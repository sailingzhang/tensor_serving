// Generated by the gRPC C++ plugin.
// If you make any local change, they will be lost.
// source: tensorflow_serving/apis/prediction_service.proto
#ifndef GRPC_tensorflow_5fserving_2fapis_2fprediction_5fservice_2eproto__INCLUDED
#define GRPC_tensorflow_5fserving_2fapis_2fprediction_5fservice_2eproto__INCLUDED

#include "tensorflow_serving/apis/prediction_service.pb.h"

#include <functional>
#include <grpc/impl/codegen/port_platform.h>
#include <grpcpp/impl/codegen/async_generic_service.h>
#include <grpcpp/impl/codegen/async_stream.h>
#include <grpcpp/impl/codegen/async_unary_call.h>
#include <grpcpp/impl/codegen/client_callback.h>
#include <grpcpp/impl/codegen/client_context.h>
#include <grpcpp/impl/codegen/completion_queue.h>
#include <grpcpp/impl/codegen/message_allocator.h>
#include <grpcpp/impl/codegen/method_handler.h>
#include <grpcpp/impl/codegen/proto_utils.h>
#include <grpcpp/impl/codegen/rpc_method.h>
#include <grpcpp/impl/codegen/server_callback.h>
#include <grpcpp/impl/codegen/server_callback_handlers.h>
#include <grpcpp/impl/codegen/server_context.h>
#include <grpcpp/impl/codegen/service_type.h>
#include <grpcpp/impl/codegen/status.h>
#include <grpcpp/impl/codegen/stub_options.h>
#include <grpcpp/impl/codegen/sync_stream.h>

namespace tensorflow {
namespace serving {

// open source marker; do not remove
// PredictionService provides access to machine-learned models loaded by
// model_servers.
class PredictionService final {
 public:
  static constexpr char const* service_full_name() {
    return "tensorflow.serving.PredictionService";
  }
  class StubInterface {
   public:
    virtual ~StubInterface() {}
    // Classify.
    virtual ::grpc::Status Classify(::grpc::ClientContext* context, const ::tensorflow::serving::ClassificationRequest& request, ::tensorflow::serving::ClassificationResponse* response) = 0;
    std::unique_ptr< ::grpc::ClientAsyncResponseReaderInterface< ::tensorflow::serving::ClassificationResponse>> AsyncClassify(::grpc::ClientContext* context, const ::tensorflow::serving::ClassificationRequest& request, ::grpc::CompletionQueue* cq) {
      return std::unique_ptr< ::grpc::ClientAsyncResponseReaderInterface< ::tensorflow::serving::ClassificationResponse>>(AsyncClassifyRaw(context, request, cq));
    }
    std::unique_ptr< ::grpc::ClientAsyncResponseReaderInterface< ::tensorflow::serving::ClassificationResponse>> PrepareAsyncClassify(::grpc::ClientContext* context, const ::tensorflow::serving::ClassificationRequest& request, ::grpc::CompletionQueue* cq) {
      return std::unique_ptr< ::grpc::ClientAsyncResponseReaderInterface< ::tensorflow::serving::ClassificationResponse>>(PrepareAsyncClassifyRaw(context, request, cq));
    }
    // Regress.
    virtual ::grpc::Status Regress(::grpc::ClientContext* context, const ::tensorflow::serving::RegressionRequest& request, ::tensorflow::serving::RegressionResponse* response) = 0;
    std::unique_ptr< ::grpc::ClientAsyncResponseReaderInterface< ::tensorflow::serving::RegressionResponse>> AsyncRegress(::grpc::ClientContext* context, const ::tensorflow::serving::RegressionRequest& request, ::grpc::CompletionQueue* cq) {
      return std::unique_ptr< ::grpc::ClientAsyncResponseReaderInterface< ::tensorflow::serving::RegressionResponse>>(AsyncRegressRaw(context, request, cq));
    }
    std::unique_ptr< ::grpc::ClientAsyncResponseReaderInterface< ::tensorflow::serving::RegressionResponse>> PrepareAsyncRegress(::grpc::ClientContext* context, const ::tensorflow::serving::RegressionRequest& request, ::grpc::CompletionQueue* cq) {
      return std::unique_ptr< ::grpc::ClientAsyncResponseReaderInterface< ::tensorflow::serving::RegressionResponse>>(PrepareAsyncRegressRaw(context, request, cq));
    }
    // Predict -- provides access to loaded TensorFlow model.
    virtual ::grpc::Status Predict(::grpc::ClientContext* context, const ::tensorflow::serving::PredictRequest& request, ::tensorflow::serving::PredictResponse* response) = 0;
    std::unique_ptr< ::grpc::ClientAsyncResponseReaderInterface< ::tensorflow::serving::PredictResponse>> AsyncPredict(::grpc::ClientContext* context, const ::tensorflow::serving::PredictRequest& request, ::grpc::CompletionQueue* cq) {
      return std::unique_ptr< ::grpc::ClientAsyncResponseReaderInterface< ::tensorflow::serving::PredictResponse>>(AsyncPredictRaw(context, request, cq));
    }
    std::unique_ptr< ::grpc::ClientAsyncResponseReaderInterface< ::tensorflow::serving::PredictResponse>> PrepareAsyncPredict(::grpc::ClientContext* context, const ::tensorflow::serving::PredictRequest& request, ::grpc::CompletionQueue* cq) {
      return std::unique_ptr< ::grpc::ClientAsyncResponseReaderInterface< ::tensorflow::serving::PredictResponse>>(PrepareAsyncPredictRaw(context, request, cq));
    }
    // MultiInference API for multi-headed models.
    virtual ::grpc::Status MultiInference(::grpc::ClientContext* context, const ::tensorflow::serving::MultiInferenceRequest& request, ::tensorflow::serving::MultiInferenceResponse* response) = 0;
    std::unique_ptr< ::grpc::ClientAsyncResponseReaderInterface< ::tensorflow::serving::MultiInferenceResponse>> AsyncMultiInference(::grpc::ClientContext* context, const ::tensorflow::serving::MultiInferenceRequest& request, ::grpc::CompletionQueue* cq) {
      return std::unique_ptr< ::grpc::ClientAsyncResponseReaderInterface< ::tensorflow::serving::MultiInferenceResponse>>(AsyncMultiInferenceRaw(context, request, cq));
    }
    std::unique_ptr< ::grpc::ClientAsyncResponseReaderInterface< ::tensorflow::serving::MultiInferenceResponse>> PrepareAsyncMultiInference(::grpc::ClientContext* context, const ::tensorflow::serving::MultiInferenceRequest& request, ::grpc::CompletionQueue* cq) {
      return std::unique_ptr< ::grpc::ClientAsyncResponseReaderInterface< ::tensorflow::serving::MultiInferenceResponse>>(PrepareAsyncMultiInferenceRaw(context, request, cq));
    }
    // GetModelMetadata - provides access to metadata for loaded models.
    virtual ::grpc::Status GetModelMetadata(::grpc::ClientContext* context, const ::tensorflow::serving::GetModelMetadataRequest& request, ::tensorflow::serving::GetModelMetadataResponse* response) = 0;
    std::unique_ptr< ::grpc::ClientAsyncResponseReaderInterface< ::tensorflow::serving::GetModelMetadataResponse>> AsyncGetModelMetadata(::grpc::ClientContext* context, const ::tensorflow::serving::GetModelMetadataRequest& request, ::grpc::CompletionQueue* cq) {
      return std::unique_ptr< ::grpc::ClientAsyncResponseReaderInterface< ::tensorflow::serving::GetModelMetadataResponse>>(AsyncGetModelMetadataRaw(context, request, cq));
    }
    std::unique_ptr< ::grpc::ClientAsyncResponseReaderInterface< ::tensorflow::serving::GetModelMetadataResponse>> PrepareAsyncGetModelMetadata(::grpc::ClientContext* context, const ::tensorflow::serving::GetModelMetadataRequest& request, ::grpc::CompletionQueue* cq) {
      return std::unique_ptr< ::grpc::ClientAsyncResponseReaderInterface< ::tensorflow::serving::GetModelMetadataResponse>>(PrepareAsyncGetModelMetadataRaw(context, request, cq));
    }
    class experimental_async_interface {
     public:
      virtual ~experimental_async_interface() {}
      // Classify.
      virtual void Classify(::grpc::ClientContext* context, const ::tensorflow::serving::ClassificationRequest* request, ::tensorflow::serving::ClassificationResponse* response, std::function<void(::grpc::Status)>) = 0;
      virtual void Classify(::grpc::ClientContext* context, const ::grpc::ByteBuffer* request, ::tensorflow::serving::ClassificationResponse* response, std::function<void(::grpc::Status)>) = 0;
      #ifdef GRPC_CALLBACK_API_NONEXPERIMENTAL
      virtual void Classify(::grpc::ClientContext* context, const ::tensorflow::serving::ClassificationRequest* request, ::tensorflow::serving::ClassificationResponse* response, ::grpc::ClientUnaryReactor* reactor) = 0;
      #else
      virtual void Classify(::grpc::ClientContext* context, const ::tensorflow::serving::ClassificationRequest* request, ::tensorflow::serving::ClassificationResponse* response, ::grpc::experimental::ClientUnaryReactor* reactor) = 0;
      #endif
      #ifdef GRPC_CALLBACK_API_NONEXPERIMENTAL
      virtual void Classify(::grpc::ClientContext* context, const ::grpc::ByteBuffer* request, ::tensorflow::serving::ClassificationResponse* response, ::grpc::ClientUnaryReactor* reactor) = 0;
      #else
      virtual void Classify(::grpc::ClientContext* context, const ::grpc::ByteBuffer* request, ::tensorflow::serving::ClassificationResponse* response, ::grpc::experimental::ClientUnaryReactor* reactor) = 0;
      #endif
      // Regress.
      virtual void Regress(::grpc::ClientContext* context, const ::tensorflow::serving::RegressionRequest* request, ::tensorflow::serving::RegressionResponse* response, std::function<void(::grpc::Status)>) = 0;
      virtual void Regress(::grpc::ClientContext* context, const ::grpc::ByteBuffer* request, ::tensorflow::serving::RegressionResponse* response, std::function<void(::grpc::Status)>) = 0;
      #ifdef GRPC_CALLBACK_API_NONEXPERIMENTAL
      virtual void Regress(::grpc::ClientContext* context, const ::tensorflow::serving::RegressionRequest* request, ::tensorflow::serving::RegressionResponse* response, ::grpc::ClientUnaryReactor* reactor) = 0;
      #else
      virtual void Regress(::grpc::ClientContext* context, const ::tensorflow::serving::RegressionRequest* request, ::tensorflow::serving::RegressionResponse* response, ::grpc::experimental::ClientUnaryReactor* reactor) = 0;
      #endif
      #ifdef GRPC_CALLBACK_API_NONEXPERIMENTAL
      virtual void Regress(::grpc::ClientContext* context, const ::grpc::ByteBuffer* request, ::tensorflow::serving::RegressionResponse* response, ::grpc::ClientUnaryReactor* reactor) = 0;
      #else
      virtual void Regress(::grpc::ClientContext* context, const ::grpc::ByteBuffer* request, ::tensorflow::serving::RegressionResponse* response, ::grpc::experimental::ClientUnaryReactor* reactor) = 0;
      #endif
      // Predict -- provides access to loaded TensorFlow model.
      virtual void Predict(::grpc::ClientContext* context, const ::tensorflow::serving::PredictRequest* request, ::tensorflow::serving::PredictResponse* response, std::function<void(::grpc::Status)>) = 0;
      virtual void Predict(::grpc::ClientContext* context, const ::grpc::ByteBuffer* request, ::tensorflow::serving::PredictResponse* response, std::function<void(::grpc::Status)>) = 0;
      #ifdef GRPC_CALLBACK_API_NONEXPERIMENTAL
      virtual void Predict(::grpc::ClientContext* context, const ::tensorflow::serving::PredictRequest* request, ::tensorflow::serving::PredictResponse* response, ::grpc::ClientUnaryReactor* reactor) = 0;
      #else
      virtual void Predict(::grpc::ClientContext* context, const ::tensorflow::serving::PredictRequest* request, ::tensorflow::serving::PredictResponse* response, ::grpc::experimental::ClientUnaryReactor* reactor) = 0;
      #endif
      #ifdef GRPC_CALLBACK_API_NONEXPERIMENTAL
      virtual void Predict(::grpc::ClientContext* context, const ::grpc::ByteBuffer* request, ::tensorflow::serving::PredictResponse* response, ::grpc::ClientUnaryReactor* reactor) = 0;
      #else
      virtual void Predict(::grpc::ClientContext* context, const ::grpc::ByteBuffer* request, ::tensorflow::serving::PredictResponse* response, ::grpc::experimental::ClientUnaryReactor* reactor) = 0;
      #endif
      // MultiInference API for multi-headed models.
      virtual void MultiInference(::grpc::ClientContext* context, const ::tensorflow::serving::MultiInferenceRequest* request, ::tensorflow::serving::MultiInferenceResponse* response, std::function<void(::grpc::Status)>) = 0;
      virtual void MultiInference(::grpc::ClientContext* context, const ::grpc::ByteBuffer* request, ::tensorflow::serving::MultiInferenceResponse* response, std::function<void(::grpc::Status)>) = 0;
      #ifdef GRPC_CALLBACK_API_NONEXPERIMENTAL
      virtual void MultiInference(::grpc::ClientContext* context, const ::tensorflow::serving::MultiInferenceRequest* request, ::tensorflow::serving::MultiInferenceResponse* response, ::grpc::ClientUnaryReactor* reactor) = 0;
      #else
      virtual void MultiInference(::grpc::ClientContext* context, const ::tensorflow::serving::MultiInferenceRequest* request, ::tensorflow::serving::MultiInferenceResponse* response, ::grpc::experimental::ClientUnaryReactor* reactor) = 0;
      #endif
      #ifdef GRPC_CALLBACK_API_NONEXPERIMENTAL
      virtual void MultiInference(::grpc::ClientContext* context, const ::grpc::ByteBuffer* request, ::tensorflow::serving::MultiInferenceResponse* response, ::grpc::ClientUnaryReactor* reactor) = 0;
      #else
      virtual void MultiInference(::grpc::ClientContext* context, const ::grpc::ByteBuffer* request, ::tensorflow::serving::MultiInferenceResponse* response, ::grpc::experimental::ClientUnaryReactor* reactor) = 0;
      #endif
      // GetModelMetadata - provides access to metadata for loaded models.
      virtual void GetModelMetadata(::grpc::ClientContext* context, const ::tensorflow::serving::GetModelMetadataRequest* request, ::tensorflow::serving::GetModelMetadataResponse* response, std::function<void(::grpc::Status)>) = 0;
      virtual void GetModelMetadata(::grpc::ClientContext* context, const ::grpc::ByteBuffer* request, ::tensorflow::serving::GetModelMetadataResponse* response, std::function<void(::grpc::Status)>) = 0;
      #ifdef GRPC_CALLBACK_API_NONEXPERIMENTAL
      virtual void GetModelMetadata(::grpc::ClientContext* context, const ::tensorflow::serving::GetModelMetadataRequest* request, ::tensorflow::serving::GetModelMetadataResponse* response, ::grpc::ClientUnaryReactor* reactor) = 0;
      #else
      virtual void GetModelMetadata(::grpc::ClientContext* context, const ::tensorflow::serving::GetModelMetadataRequest* request, ::tensorflow::serving::GetModelMetadataResponse* response, ::grpc::experimental::ClientUnaryReactor* reactor) = 0;
      #endif
      #ifdef GRPC_CALLBACK_API_NONEXPERIMENTAL
      virtual void GetModelMetadata(::grpc::ClientContext* context, const ::grpc::ByteBuffer* request, ::tensorflow::serving::GetModelMetadataResponse* response, ::grpc::ClientUnaryReactor* reactor) = 0;
      #else
      virtual void GetModelMetadata(::grpc::ClientContext* context, const ::grpc::ByteBuffer* request, ::tensorflow::serving::GetModelMetadataResponse* response, ::grpc::experimental::ClientUnaryReactor* reactor) = 0;
      #endif
    };
    #ifdef GRPC_CALLBACK_API_NONEXPERIMENTAL
    typedef class experimental_async_interface async_interface;
    #endif
    #ifdef GRPC_CALLBACK_API_NONEXPERIMENTAL
    async_interface* async() { return experimental_async(); }
    #endif
    virtual class experimental_async_interface* experimental_async() { return nullptr; }
  private:
    virtual ::grpc::ClientAsyncResponseReaderInterface< ::tensorflow::serving::ClassificationResponse>* AsyncClassifyRaw(::grpc::ClientContext* context, const ::tensorflow::serving::ClassificationRequest& request, ::grpc::CompletionQueue* cq) = 0;
    virtual ::grpc::ClientAsyncResponseReaderInterface< ::tensorflow::serving::ClassificationResponse>* PrepareAsyncClassifyRaw(::grpc::ClientContext* context, const ::tensorflow::serving::ClassificationRequest& request, ::grpc::CompletionQueue* cq) = 0;
    virtual ::grpc::ClientAsyncResponseReaderInterface< ::tensorflow::serving::RegressionResponse>* AsyncRegressRaw(::grpc::ClientContext* context, const ::tensorflow::serving::RegressionRequest& request, ::grpc::CompletionQueue* cq) = 0;
    virtual ::grpc::ClientAsyncResponseReaderInterface< ::tensorflow::serving::RegressionResponse>* PrepareAsyncRegressRaw(::grpc::ClientContext* context, const ::tensorflow::serving::RegressionRequest& request, ::grpc::CompletionQueue* cq) = 0;
    virtual ::grpc::ClientAsyncResponseReaderInterface< ::tensorflow::serving::PredictResponse>* AsyncPredictRaw(::grpc::ClientContext* context, const ::tensorflow::serving::PredictRequest& request, ::grpc::CompletionQueue* cq) = 0;
    virtual ::grpc::ClientAsyncResponseReaderInterface< ::tensorflow::serving::PredictResponse>* PrepareAsyncPredictRaw(::grpc::ClientContext* context, const ::tensorflow::serving::PredictRequest& request, ::grpc::CompletionQueue* cq) = 0;
    virtual ::grpc::ClientAsyncResponseReaderInterface< ::tensorflow::serving::MultiInferenceResponse>* AsyncMultiInferenceRaw(::grpc::ClientContext* context, const ::tensorflow::serving::MultiInferenceRequest& request, ::grpc::CompletionQueue* cq) = 0;
    virtual ::grpc::ClientAsyncResponseReaderInterface< ::tensorflow::serving::MultiInferenceResponse>* PrepareAsyncMultiInferenceRaw(::grpc::ClientContext* context, const ::tensorflow::serving::MultiInferenceRequest& request, ::grpc::CompletionQueue* cq) = 0;
    virtual ::grpc::ClientAsyncResponseReaderInterface< ::tensorflow::serving::GetModelMetadataResponse>* AsyncGetModelMetadataRaw(::grpc::ClientContext* context, const ::tensorflow::serving::GetModelMetadataRequest& request, ::grpc::CompletionQueue* cq) = 0;
    virtual ::grpc::ClientAsyncResponseReaderInterface< ::tensorflow::serving::GetModelMetadataResponse>* PrepareAsyncGetModelMetadataRaw(::grpc::ClientContext* context, const ::tensorflow::serving::GetModelMetadataRequest& request, ::grpc::CompletionQueue* cq) = 0;
  };
  class Stub final : public StubInterface {
   public:
    Stub(const std::shared_ptr< ::grpc::ChannelInterface>& channel);
    ::grpc::Status Classify(::grpc::ClientContext* context, const ::tensorflow::serving::ClassificationRequest& request, ::tensorflow::serving::ClassificationResponse* response) override;
    std::unique_ptr< ::grpc::ClientAsyncResponseReader< ::tensorflow::serving::ClassificationResponse>> AsyncClassify(::grpc::ClientContext* context, const ::tensorflow::serving::ClassificationRequest& request, ::grpc::CompletionQueue* cq) {
      return std::unique_ptr< ::grpc::ClientAsyncResponseReader< ::tensorflow::serving::ClassificationResponse>>(AsyncClassifyRaw(context, request, cq));
    }
    std::unique_ptr< ::grpc::ClientAsyncResponseReader< ::tensorflow::serving::ClassificationResponse>> PrepareAsyncClassify(::grpc::ClientContext* context, const ::tensorflow::serving::ClassificationRequest& request, ::grpc::CompletionQueue* cq) {
      return std::unique_ptr< ::grpc::ClientAsyncResponseReader< ::tensorflow::serving::ClassificationResponse>>(PrepareAsyncClassifyRaw(context, request, cq));
    }
    ::grpc::Status Regress(::grpc::ClientContext* context, const ::tensorflow::serving::RegressionRequest& request, ::tensorflow::serving::RegressionResponse* response) override;
    std::unique_ptr< ::grpc::ClientAsyncResponseReader< ::tensorflow::serving::RegressionResponse>> AsyncRegress(::grpc::ClientContext* context, const ::tensorflow::serving::RegressionRequest& request, ::grpc::CompletionQueue* cq) {
      return std::unique_ptr< ::grpc::ClientAsyncResponseReader< ::tensorflow::serving::RegressionResponse>>(AsyncRegressRaw(context, request, cq));
    }
    std::unique_ptr< ::grpc::ClientAsyncResponseReader< ::tensorflow::serving::RegressionResponse>> PrepareAsyncRegress(::grpc::ClientContext* context, const ::tensorflow::serving::RegressionRequest& request, ::grpc::CompletionQueue* cq) {
      return std::unique_ptr< ::grpc::ClientAsyncResponseReader< ::tensorflow::serving::RegressionResponse>>(PrepareAsyncRegressRaw(context, request, cq));
    }
    ::grpc::Status Predict(::grpc::ClientContext* context, const ::tensorflow::serving::PredictRequest& request, ::tensorflow::serving::PredictResponse* response) override;
    std::unique_ptr< ::grpc::ClientAsyncResponseReader< ::tensorflow::serving::PredictResponse>> AsyncPredict(::grpc::ClientContext* context, const ::tensorflow::serving::PredictRequest& request, ::grpc::CompletionQueue* cq) {
      return std::unique_ptr< ::grpc::ClientAsyncResponseReader< ::tensorflow::serving::PredictResponse>>(AsyncPredictRaw(context, request, cq));
    }
    std::unique_ptr< ::grpc::ClientAsyncResponseReader< ::tensorflow::serving::PredictResponse>> PrepareAsyncPredict(::grpc::ClientContext* context, const ::tensorflow::serving::PredictRequest& request, ::grpc::CompletionQueue* cq) {
      return std::unique_ptr< ::grpc::ClientAsyncResponseReader< ::tensorflow::serving::PredictResponse>>(PrepareAsyncPredictRaw(context, request, cq));
    }
    ::grpc::Status MultiInference(::grpc::ClientContext* context, const ::tensorflow::serving::MultiInferenceRequest& request, ::tensorflow::serving::MultiInferenceResponse* response) override;
    std::unique_ptr< ::grpc::ClientAsyncResponseReader< ::tensorflow::serving::MultiInferenceResponse>> AsyncMultiInference(::grpc::ClientContext* context, const ::tensorflow::serving::MultiInferenceRequest& request, ::grpc::CompletionQueue* cq) {
      return std::unique_ptr< ::grpc::ClientAsyncResponseReader< ::tensorflow::serving::MultiInferenceResponse>>(AsyncMultiInferenceRaw(context, request, cq));
    }
    std::unique_ptr< ::grpc::ClientAsyncResponseReader< ::tensorflow::serving::MultiInferenceResponse>> PrepareAsyncMultiInference(::grpc::ClientContext* context, const ::tensorflow::serving::MultiInferenceRequest& request, ::grpc::CompletionQueue* cq) {
      return std::unique_ptr< ::grpc::ClientAsyncResponseReader< ::tensorflow::serving::MultiInferenceResponse>>(PrepareAsyncMultiInferenceRaw(context, request, cq));
    }
    ::grpc::Status GetModelMetadata(::grpc::ClientContext* context, const ::tensorflow::serving::GetModelMetadataRequest& request, ::tensorflow::serving::GetModelMetadataResponse* response) override;
    std::unique_ptr< ::grpc::ClientAsyncResponseReader< ::tensorflow::serving::GetModelMetadataResponse>> AsyncGetModelMetadata(::grpc::ClientContext* context, const ::tensorflow::serving::GetModelMetadataRequest& request, ::grpc::CompletionQueue* cq) {
      return std::unique_ptr< ::grpc::ClientAsyncResponseReader< ::tensorflow::serving::GetModelMetadataResponse>>(AsyncGetModelMetadataRaw(context, request, cq));
    }
    std::unique_ptr< ::grpc::ClientAsyncResponseReader< ::tensorflow::serving::GetModelMetadataResponse>> PrepareAsyncGetModelMetadata(::grpc::ClientContext* context, const ::tensorflow::serving::GetModelMetadataRequest& request, ::grpc::CompletionQueue* cq) {
      return std::unique_ptr< ::grpc::ClientAsyncResponseReader< ::tensorflow::serving::GetModelMetadataResponse>>(PrepareAsyncGetModelMetadataRaw(context, request, cq));
    }
    class experimental_async final :
      public StubInterface::experimental_async_interface {
     public:
      void Classify(::grpc::ClientContext* context, const ::tensorflow::serving::ClassificationRequest* request, ::tensorflow::serving::ClassificationResponse* response, std::function<void(::grpc::Status)>) override;
      void Classify(::grpc::ClientContext* context, const ::grpc::ByteBuffer* request, ::tensorflow::serving::ClassificationResponse* response, std::function<void(::grpc::Status)>) override;
      #ifdef GRPC_CALLBACK_API_NONEXPERIMENTAL
      void Classify(::grpc::ClientContext* context, const ::tensorflow::serving::ClassificationRequest* request, ::tensorflow::serving::ClassificationResponse* response, ::grpc::ClientUnaryReactor* reactor) override;
      #else
      void Classify(::grpc::ClientContext* context, const ::tensorflow::serving::ClassificationRequest* request, ::tensorflow::serving::ClassificationResponse* response, ::grpc::experimental::ClientUnaryReactor* reactor) override;
      #endif
      #ifdef GRPC_CALLBACK_API_NONEXPERIMENTAL
      void Classify(::grpc::ClientContext* context, const ::grpc::ByteBuffer* request, ::tensorflow::serving::ClassificationResponse* response, ::grpc::ClientUnaryReactor* reactor) override;
      #else
      void Classify(::grpc::ClientContext* context, const ::grpc::ByteBuffer* request, ::tensorflow::serving::ClassificationResponse* response, ::grpc::experimental::ClientUnaryReactor* reactor) override;
      #endif
      void Regress(::grpc::ClientContext* context, const ::tensorflow::serving::RegressionRequest* request, ::tensorflow::serving::RegressionResponse* response, std::function<void(::grpc::Status)>) override;
      void Regress(::grpc::ClientContext* context, const ::grpc::ByteBuffer* request, ::tensorflow::serving::RegressionResponse* response, std::function<void(::grpc::Status)>) override;
      #ifdef GRPC_CALLBACK_API_NONEXPERIMENTAL
      void Regress(::grpc::ClientContext* context, const ::tensorflow::serving::RegressionRequest* request, ::tensorflow::serving::RegressionResponse* response, ::grpc::ClientUnaryReactor* reactor) override;
      #else
      void Regress(::grpc::ClientContext* context, const ::tensorflow::serving::RegressionRequest* request, ::tensorflow::serving::RegressionResponse* response, ::grpc::experimental::ClientUnaryReactor* reactor) override;
      #endif
      #ifdef GRPC_CALLBACK_API_NONEXPERIMENTAL
      void Regress(::grpc::ClientContext* context, const ::grpc::ByteBuffer* request, ::tensorflow::serving::RegressionResponse* response, ::grpc::ClientUnaryReactor* reactor) override;
      #else
      void Regress(::grpc::ClientContext* context, const ::grpc::ByteBuffer* request, ::tensorflow::serving::RegressionResponse* response, ::grpc::experimental::ClientUnaryReactor* reactor) override;
      #endif
      void Predict(::grpc::ClientContext* context, const ::tensorflow::serving::PredictRequest* request, ::tensorflow::serving::PredictResponse* response, std::function<void(::grpc::Status)>) override;
      void Predict(::grpc::ClientContext* context, const ::grpc::ByteBuffer* request, ::tensorflow::serving::PredictResponse* response, std::function<void(::grpc::Status)>) override;
      #ifdef GRPC_CALLBACK_API_NONEXPERIMENTAL
      void Predict(::grpc::ClientContext* context, const ::tensorflow::serving::PredictRequest* request, ::tensorflow::serving::PredictResponse* response, ::grpc::ClientUnaryReactor* reactor) override;
      #else
      void Predict(::grpc::ClientContext* context, const ::tensorflow::serving::PredictRequest* request, ::tensorflow::serving::PredictResponse* response, ::grpc::experimental::ClientUnaryReactor* reactor) override;
      #endif
      #ifdef GRPC_CALLBACK_API_NONEXPERIMENTAL
      void Predict(::grpc::ClientContext* context, const ::grpc::ByteBuffer* request, ::tensorflow::serving::PredictResponse* response, ::grpc::ClientUnaryReactor* reactor) override;
      #else
      void Predict(::grpc::ClientContext* context, const ::grpc::ByteBuffer* request, ::tensorflow::serving::PredictResponse* response, ::grpc::experimental::ClientUnaryReactor* reactor) override;
      #endif
      void MultiInference(::grpc::ClientContext* context, const ::tensorflow::serving::MultiInferenceRequest* request, ::tensorflow::serving::MultiInferenceResponse* response, std::function<void(::grpc::Status)>) override;
      void MultiInference(::grpc::ClientContext* context, const ::grpc::ByteBuffer* request, ::tensorflow::serving::MultiInferenceResponse* response, std::function<void(::grpc::Status)>) override;
      #ifdef GRPC_CALLBACK_API_NONEXPERIMENTAL
      void MultiInference(::grpc::ClientContext* context, const ::tensorflow::serving::MultiInferenceRequest* request, ::tensorflow::serving::MultiInferenceResponse* response, ::grpc::ClientUnaryReactor* reactor) override;
      #else
      void MultiInference(::grpc::ClientContext* context, const ::tensorflow::serving::MultiInferenceRequest* request, ::tensorflow::serving::MultiInferenceResponse* response, ::grpc::experimental::ClientUnaryReactor* reactor) override;
      #endif
      #ifdef GRPC_CALLBACK_API_NONEXPERIMENTAL
      void MultiInference(::grpc::ClientContext* context, const ::grpc::ByteBuffer* request, ::tensorflow::serving::MultiInferenceResponse* response, ::grpc::ClientUnaryReactor* reactor) override;
      #else
      void MultiInference(::grpc::ClientContext* context, const ::grpc::ByteBuffer* request, ::tensorflow::serving::MultiInferenceResponse* response, ::grpc::experimental::ClientUnaryReactor* reactor) override;
      #endif
      void GetModelMetadata(::grpc::ClientContext* context, const ::tensorflow::serving::GetModelMetadataRequest* request, ::tensorflow::serving::GetModelMetadataResponse* response, std::function<void(::grpc::Status)>) override;
      void GetModelMetadata(::grpc::ClientContext* context, const ::grpc::ByteBuffer* request, ::tensorflow::serving::GetModelMetadataResponse* response, std::function<void(::grpc::Status)>) override;
      #ifdef GRPC_CALLBACK_API_NONEXPERIMENTAL
      void GetModelMetadata(::grpc::ClientContext* context, const ::tensorflow::serving::GetModelMetadataRequest* request, ::tensorflow::serving::GetModelMetadataResponse* response, ::grpc::ClientUnaryReactor* reactor) override;
      #else
      void GetModelMetadata(::grpc::ClientContext* context, const ::tensorflow::serving::GetModelMetadataRequest* request, ::tensorflow::serving::GetModelMetadataResponse* response, ::grpc::experimental::ClientUnaryReactor* reactor) override;
      #endif
      #ifdef GRPC_CALLBACK_API_NONEXPERIMENTAL
      void GetModelMetadata(::grpc::ClientContext* context, const ::grpc::ByteBuffer* request, ::tensorflow::serving::GetModelMetadataResponse* response, ::grpc::ClientUnaryReactor* reactor) override;
      #else
      void GetModelMetadata(::grpc::ClientContext* context, const ::grpc::ByteBuffer* request, ::tensorflow::serving::GetModelMetadataResponse* response, ::grpc::experimental::ClientUnaryReactor* reactor) override;
      #endif
     private:
      friend class Stub;
      explicit experimental_async(Stub* stub): stub_(stub) { }
      Stub* stub() { return stub_; }
      Stub* stub_;
    };
    class experimental_async_interface* experimental_async() override { return &async_stub_; }

   private:
    std::shared_ptr< ::grpc::ChannelInterface> channel_;
    class experimental_async async_stub_{this};
    ::grpc::ClientAsyncResponseReader< ::tensorflow::serving::ClassificationResponse>* AsyncClassifyRaw(::grpc::ClientContext* context, const ::tensorflow::serving::ClassificationRequest& request, ::grpc::CompletionQueue* cq) override;
    ::grpc::ClientAsyncResponseReader< ::tensorflow::serving::ClassificationResponse>* PrepareAsyncClassifyRaw(::grpc::ClientContext* context, const ::tensorflow::serving::ClassificationRequest& request, ::grpc::CompletionQueue* cq) override;
    ::grpc::ClientAsyncResponseReader< ::tensorflow::serving::RegressionResponse>* AsyncRegressRaw(::grpc::ClientContext* context, const ::tensorflow::serving::RegressionRequest& request, ::grpc::CompletionQueue* cq) override;
    ::grpc::ClientAsyncResponseReader< ::tensorflow::serving::RegressionResponse>* PrepareAsyncRegressRaw(::grpc::ClientContext* context, const ::tensorflow::serving::RegressionRequest& request, ::grpc::CompletionQueue* cq) override;
    ::grpc::ClientAsyncResponseReader< ::tensorflow::serving::PredictResponse>* AsyncPredictRaw(::grpc::ClientContext* context, const ::tensorflow::serving::PredictRequest& request, ::grpc::CompletionQueue* cq) override;
    ::grpc::ClientAsyncResponseReader< ::tensorflow::serving::PredictResponse>* PrepareAsyncPredictRaw(::grpc::ClientContext* context, const ::tensorflow::serving::PredictRequest& request, ::grpc::CompletionQueue* cq) override;
    ::grpc::ClientAsyncResponseReader< ::tensorflow::serving::MultiInferenceResponse>* AsyncMultiInferenceRaw(::grpc::ClientContext* context, const ::tensorflow::serving::MultiInferenceRequest& request, ::grpc::CompletionQueue* cq) override;
    ::grpc::ClientAsyncResponseReader< ::tensorflow::serving::MultiInferenceResponse>* PrepareAsyncMultiInferenceRaw(::grpc::ClientContext* context, const ::tensorflow::serving::MultiInferenceRequest& request, ::grpc::CompletionQueue* cq) override;
    ::grpc::ClientAsyncResponseReader< ::tensorflow::serving::GetModelMetadataResponse>* AsyncGetModelMetadataRaw(::grpc::ClientContext* context, const ::tensorflow::serving::GetModelMetadataRequest& request, ::grpc::CompletionQueue* cq) override;
    ::grpc::ClientAsyncResponseReader< ::tensorflow::serving::GetModelMetadataResponse>* PrepareAsyncGetModelMetadataRaw(::grpc::ClientContext* context, const ::tensorflow::serving::GetModelMetadataRequest& request, ::grpc::CompletionQueue* cq) override;
    const ::grpc::internal::RpcMethod rpcmethod_Classify_;
    const ::grpc::internal::RpcMethod rpcmethod_Regress_;
    const ::grpc::internal::RpcMethod rpcmethod_Predict_;
    const ::grpc::internal::RpcMethod rpcmethod_MultiInference_;
    const ::grpc::internal::RpcMethod rpcmethod_GetModelMetadata_;
  };
  static std::unique_ptr<Stub> NewStub(const std::shared_ptr< ::grpc::ChannelInterface>& channel, const ::grpc::StubOptions& options = ::grpc::StubOptions());

  class Service : public ::grpc::Service {
   public:
    Service();
    virtual ~Service();
    // Classify.
    virtual ::grpc::Status Classify(::grpc::ServerContext* context, const ::tensorflow::serving::ClassificationRequest* request, ::tensorflow::serving::ClassificationResponse* response);
    // Regress.
    virtual ::grpc::Status Regress(::grpc::ServerContext* context, const ::tensorflow::serving::RegressionRequest* request, ::tensorflow::serving::RegressionResponse* response);
    // Predict -- provides access to loaded TensorFlow model.
    virtual ::grpc::Status Predict(::grpc::ServerContext* context, const ::tensorflow::serving::PredictRequest* request, ::tensorflow::serving::PredictResponse* response);
    // MultiInference API for multi-headed models.
    virtual ::grpc::Status MultiInference(::grpc::ServerContext* context, const ::tensorflow::serving::MultiInferenceRequest* request, ::tensorflow::serving::MultiInferenceResponse* response);
    // GetModelMetadata - provides access to metadata for loaded models.
    virtual ::grpc::Status GetModelMetadata(::grpc::ServerContext* context, const ::tensorflow::serving::GetModelMetadataRequest* request, ::tensorflow::serving::GetModelMetadataResponse* response);
  };
  template <class BaseClass>
  class WithAsyncMethod_Classify : public BaseClass {
   private:
    void BaseClassMustBeDerivedFromService(const Service* /*service*/) {}
   public:
    WithAsyncMethod_Classify() {
      ::grpc::Service::MarkMethodAsync(0);
    }
    ~WithAsyncMethod_Classify() override {
      BaseClassMustBeDerivedFromService(this);
    }
    // disable synchronous version of this method
    ::grpc::Status Classify(::grpc::ServerContext* /*context*/, const ::tensorflow::serving::ClassificationRequest* /*request*/, ::tensorflow::serving::ClassificationResponse* /*response*/) override {
      abort();
      return ::grpc::Status(::grpc::StatusCode::UNIMPLEMENTED, "");
    }
    void RequestClassify(::grpc::ServerContext* context, ::tensorflow::serving::ClassificationRequest* request, ::grpc::ServerAsyncResponseWriter< ::tensorflow::serving::ClassificationResponse>* response, ::grpc::CompletionQueue* new_call_cq, ::grpc::ServerCompletionQueue* notification_cq, void *tag) {
      ::grpc::Service::RequestAsyncUnary(0, context, request, response, new_call_cq, notification_cq, tag);
    }
  };
  template <class BaseClass>
  class WithAsyncMethod_Regress : public BaseClass {
   private:
    void BaseClassMustBeDerivedFromService(const Service* /*service*/) {}
   public:
    WithAsyncMethod_Regress() {
      ::grpc::Service::MarkMethodAsync(1);
    }
    ~WithAsyncMethod_Regress() override {
      BaseClassMustBeDerivedFromService(this);
    }
    // disable synchronous version of this method
    ::grpc::Status Regress(::grpc::ServerContext* /*context*/, const ::tensorflow::serving::RegressionRequest* /*request*/, ::tensorflow::serving::RegressionResponse* /*response*/) override {
      abort();
      return ::grpc::Status(::grpc::StatusCode::UNIMPLEMENTED, "");
    }
    void RequestRegress(::grpc::ServerContext* context, ::tensorflow::serving::RegressionRequest* request, ::grpc::ServerAsyncResponseWriter< ::tensorflow::serving::RegressionResponse>* response, ::grpc::CompletionQueue* new_call_cq, ::grpc::ServerCompletionQueue* notification_cq, void *tag) {
      ::grpc::Service::RequestAsyncUnary(1, context, request, response, new_call_cq, notification_cq, tag);
    }
  };
  template <class BaseClass>
  class WithAsyncMethod_Predict : public BaseClass {
   private:
    void BaseClassMustBeDerivedFromService(const Service* /*service*/) {}
   public:
    WithAsyncMethod_Predict() {
      ::grpc::Service::MarkMethodAsync(2);
    }
    ~WithAsyncMethod_Predict() override {
      BaseClassMustBeDerivedFromService(this);
    }
    // disable synchronous version of this method
    ::grpc::Status Predict(::grpc::ServerContext* /*context*/, const ::tensorflow::serving::PredictRequest* /*request*/, ::tensorflow::serving::PredictResponse* /*response*/) override {
      abort();
      return ::grpc::Status(::grpc::StatusCode::UNIMPLEMENTED, "");
    }
    void RequestPredict(::grpc::ServerContext* context, ::tensorflow::serving::PredictRequest* request, ::grpc::ServerAsyncResponseWriter< ::tensorflow::serving::PredictResponse>* response, ::grpc::CompletionQueue* new_call_cq, ::grpc::ServerCompletionQueue* notification_cq, void *tag) {
      ::grpc::Service::RequestAsyncUnary(2, context, request, response, new_call_cq, notification_cq, tag);
    }
  };
  template <class BaseClass>
  class WithAsyncMethod_MultiInference : public BaseClass {
   private:
    void BaseClassMustBeDerivedFromService(const Service* /*service*/) {}
   public:
    WithAsyncMethod_MultiInference() {
      ::grpc::Service::MarkMethodAsync(3);
    }
    ~WithAsyncMethod_MultiInference() override {
      BaseClassMustBeDerivedFromService(this);
    }
    // disable synchronous version of this method
    ::grpc::Status MultiInference(::grpc::ServerContext* /*context*/, const ::tensorflow::serving::MultiInferenceRequest* /*request*/, ::tensorflow::serving::MultiInferenceResponse* /*response*/) override {
      abort();
      return ::grpc::Status(::grpc::StatusCode::UNIMPLEMENTED, "");
    }
    void RequestMultiInference(::grpc::ServerContext* context, ::tensorflow::serving::MultiInferenceRequest* request, ::grpc::ServerAsyncResponseWriter< ::tensorflow::serving::MultiInferenceResponse>* response, ::grpc::CompletionQueue* new_call_cq, ::grpc::ServerCompletionQueue* notification_cq, void *tag) {
      ::grpc::Service::RequestAsyncUnary(3, context, request, response, new_call_cq, notification_cq, tag);
    }
  };
  template <class BaseClass>
  class WithAsyncMethod_GetModelMetadata : public BaseClass {
   private:
    void BaseClassMustBeDerivedFromService(const Service* /*service*/) {}
   public:
    WithAsyncMethod_GetModelMetadata() {
      ::grpc::Service::MarkMethodAsync(4);
    }
    ~WithAsyncMethod_GetModelMetadata() override {
      BaseClassMustBeDerivedFromService(this);
    }
    // disable synchronous version of this method
    ::grpc::Status GetModelMetadata(::grpc::ServerContext* /*context*/, const ::tensorflow::serving::GetModelMetadataRequest* /*request*/, ::tensorflow::serving::GetModelMetadataResponse* /*response*/) override {
      abort();
      return ::grpc::Status(::grpc::StatusCode::UNIMPLEMENTED, "");
    }
    void RequestGetModelMetadata(::grpc::ServerContext* context, ::tensorflow::serving::GetModelMetadataRequest* request, ::grpc::ServerAsyncResponseWriter< ::tensorflow::serving::GetModelMetadataResponse>* response, ::grpc::CompletionQueue* new_call_cq, ::grpc::ServerCompletionQueue* notification_cq, void *tag) {
      ::grpc::Service::RequestAsyncUnary(4, context, request, response, new_call_cq, notification_cq, tag);
    }
  };
  typedef WithAsyncMethod_Classify<WithAsyncMethod_Regress<WithAsyncMethod_Predict<WithAsyncMethod_MultiInference<WithAsyncMethod_GetModelMetadata<Service > > > > > AsyncService;
  template <class BaseClass>
  class ExperimentalWithCallbackMethod_Classify : public BaseClass {
   private:
    void BaseClassMustBeDerivedFromService(const Service* /*service*/) {}
   public:
    ExperimentalWithCallbackMethod_Classify() {
    #ifdef GRPC_CALLBACK_API_NONEXPERIMENTAL
      ::grpc::Service::
    #else
      ::grpc::Service::experimental().
    #endif
        MarkMethodCallback(0,
          new ::grpc_impl::internal::CallbackUnaryHandler< ::tensorflow::serving::ClassificationRequest, ::tensorflow::serving::ClassificationResponse>(
            [this](
    #ifdef GRPC_CALLBACK_API_NONEXPERIMENTAL
                   ::grpc::CallbackServerContext*
    #else
                   ::grpc::experimental::CallbackServerContext*
    #endif
                     context, const ::tensorflow::serving::ClassificationRequest* request, ::tensorflow::serving::ClassificationResponse* response) { return this->Classify(context, request, response); }));}
    void SetMessageAllocatorFor_Classify(
        ::grpc::experimental::MessageAllocator< ::tensorflow::serving::ClassificationRequest, ::tensorflow::serving::ClassificationResponse>* allocator) {
    #ifdef GRPC_CALLBACK_API_NONEXPERIMENTAL
      ::grpc::internal::MethodHandler* const handler = ::grpc::Service::GetHandler(0);
    #else
      ::grpc::internal::MethodHandler* const handler = ::grpc::Service::experimental().GetHandler(0);
    #endif
      static_cast<::grpc_impl::internal::CallbackUnaryHandler< ::tensorflow::serving::ClassificationRequest, ::tensorflow::serving::ClassificationResponse>*>(handler)
              ->SetMessageAllocator(allocator);
    }
    ~ExperimentalWithCallbackMethod_Classify() override {
      BaseClassMustBeDerivedFromService(this);
    }
    // disable synchronous version of this method
    ::grpc::Status Classify(::grpc::ServerContext* /*context*/, const ::tensorflow::serving::ClassificationRequest* /*request*/, ::tensorflow::serving::ClassificationResponse* /*response*/) override {
      abort();
      return ::grpc::Status(::grpc::StatusCode::UNIMPLEMENTED, "");
    }
    #ifdef GRPC_CALLBACK_API_NONEXPERIMENTAL
    virtual ::grpc::ServerUnaryReactor* Classify(
      ::grpc::CallbackServerContext* /*context*/, const ::tensorflow::serving::ClassificationRequest* /*request*/, ::tensorflow::serving::ClassificationResponse* /*response*/)
    #else
    virtual ::grpc::experimental::ServerUnaryReactor* Classify(
      ::grpc::experimental::CallbackServerContext* /*context*/, const ::tensorflow::serving::ClassificationRequest* /*request*/, ::tensorflow::serving::ClassificationResponse* /*response*/)
    #endif
      { return nullptr; }
  };
  template <class BaseClass>
  class ExperimentalWithCallbackMethod_Regress : public BaseClass {
   private:
    void BaseClassMustBeDerivedFromService(const Service* /*service*/) {}
   public:
    ExperimentalWithCallbackMethod_Regress() {
    #ifdef GRPC_CALLBACK_API_NONEXPERIMENTAL
      ::grpc::Service::
    #else
      ::grpc::Service::experimental().
    #endif
        MarkMethodCallback(1,
          new ::grpc_impl::internal::CallbackUnaryHandler< ::tensorflow::serving::RegressionRequest, ::tensorflow::serving::RegressionResponse>(
            [this](
    #ifdef GRPC_CALLBACK_API_NONEXPERIMENTAL
                   ::grpc::CallbackServerContext*
    #else
                   ::grpc::experimental::CallbackServerContext*
    #endif
                     context, const ::tensorflow::serving::RegressionRequest* request, ::tensorflow::serving::RegressionResponse* response) { return this->Regress(context, request, response); }));}
    void SetMessageAllocatorFor_Regress(
        ::grpc::experimental::MessageAllocator< ::tensorflow::serving::RegressionRequest, ::tensorflow::serving::RegressionResponse>* allocator) {
    #ifdef GRPC_CALLBACK_API_NONEXPERIMENTAL
      ::grpc::internal::MethodHandler* const handler = ::grpc::Service::GetHandler(1);
    #else
      ::grpc::internal::MethodHandler* const handler = ::grpc::Service::experimental().GetHandler(1);
    #endif
      static_cast<::grpc_impl::internal::CallbackUnaryHandler< ::tensorflow::serving::RegressionRequest, ::tensorflow::serving::RegressionResponse>*>(handler)
              ->SetMessageAllocator(allocator);
    }
    ~ExperimentalWithCallbackMethod_Regress() override {
      BaseClassMustBeDerivedFromService(this);
    }
    // disable synchronous version of this method
    ::grpc::Status Regress(::grpc::ServerContext* /*context*/, const ::tensorflow::serving::RegressionRequest* /*request*/, ::tensorflow::serving::RegressionResponse* /*response*/) override {
      abort();
      return ::grpc::Status(::grpc::StatusCode::UNIMPLEMENTED, "");
    }
    #ifdef GRPC_CALLBACK_API_NONEXPERIMENTAL
    virtual ::grpc::ServerUnaryReactor* Regress(
      ::grpc::CallbackServerContext* /*context*/, const ::tensorflow::serving::RegressionRequest* /*request*/, ::tensorflow::serving::RegressionResponse* /*response*/)
    #else
    virtual ::grpc::experimental::ServerUnaryReactor* Regress(
      ::grpc::experimental::CallbackServerContext* /*context*/, const ::tensorflow::serving::RegressionRequest* /*request*/, ::tensorflow::serving::RegressionResponse* /*response*/)
    #endif
      { return nullptr; }
  };
  template <class BaseClass>
  class ExperimentalWithCallbackMethod_Predict : public BaseClass {
   private:
    void BaseClassMustBeDerivedFromService(const Service* /*service*/) {}
   public:
    ExperimentalWithCallbackMethod_Predict() {
    #ifdef GRPC_CALLBACK_API_NONEXPERIMENTAL
      ::grpc::Service::
    #else
      ::grpc::Service::experimental().
    #endif
        MarkMethodCallback(2,
          new ::grpc_impl::internal::CallbackUnaryHandler< ::tensorflow::serving::PredictRequest, ::tensorflow::serving::PredictResponse>(
            [this](
    #ifdef GRPC_CALLBACK_API_NONEXPERIMENTAL
                   ::grpc::CallbackServerContext*
    #else
                   ::grpc::experimental::CallbackServerContext*
    #endif
                     context, const ::tensorflow::serving::PredictRequest* request, ::tensorflow::serving::PredictResponse* response) { return this->Predict(context, request, response); }));}
    void SetMessageAllocatorFor_Predict(
        ::grpc::experimental::MessageAllocator< ::tensorflow::serving::PredictRequest, ::tensorflow::serving::PredictResponse>* allocator) {
    #ifdef GRPC_CALLBACK_API_NONEXPERIMENTAL
      ::grpc::internal::MethodHandler* const handler = ::grpc::Service::GetHandler(2);
    #else
      ::grpc::internal::MethodHandler* const handler = ::grpc::Service::experimental().GetHandler(2);
    #endif
      static_cast<::grpc_impl::internal::CallbackUnaryHandler< ::tensorflow::serving::PredictRequest, ::tensorflow::serving::PredictResponse>*>(handler)
              ->SetMessageAllocator(allocator);
    }
    ~ExperimentalWithCallbackMethod_Predict() override {
      BaseClassMustBeDerivedFromService(this);
    }
    // disable synchronous version of this method
    ::grpc::Status Predict(::grpc::ServerContext* /*context*/, const ::tensorflow::serving::PredictRequest* /*request*/, ::tensorflow::serving::PredictResponse* /*response*/) override {
      abort();
      return ::grpc::Status(::grpc::StatusCode::UNIMPLEMENTED, "");
    }
    #ifdef GRPC_CALLBACK_API_NONEXPERIMENTAL
    virtual ::grpc::ServerUnaryReactor* Predict(
      ::grpc::CallbackServerContext* /*context*/, const ::tensorflow::serving::PredictRequest* /*request*/, ::tensorflow::serving::PredictResponse* /*response*/)
    #else
    virtual ::grpc::experimental::ServerUnaryReactor* Predict(
      ::grpc::experimental::CallbackServerContext* /*context*/, const ::tensorflow::serving::PredictRequest* /*request*/, ::tensorflow::serving::PredictResponse* /*response*/)
    #endif
      { return nullptr; }
  };
  template <class BaseClass>
  class ExperimentalWithCallbackMethod_MultiInference : public BaseClass {
   private:
    void BaseClassMustBeDerivedFromService(const Service* /*service*/) {}
   public:
    ExperimentalWithCallbackMethod_MultiInference() {
    #ifdef GRPC_CALLBACK_API_NONEXPERIMENTAL
      ::grpc::Service::
    #else
      ::grpc::Service::experimental().
    #endif
        MarkMethodCallback(3,
          new ::grpc_impl::internal::CallbackUnaryHandler< ::tensorflow::serving::MultiInferenceRequest, ::tensorflow::serving::MultiInferenceResponse>(
            [this](
    #ifdef GRPC_CALLBACK_API_NONEXPERIMENTAL
                   ::grpc::CallbackServerContext*
    #else
                   ::grpc::experimental::CallbackServerContext*
    #endif
                     context, const ::tensorflow::serving::MultiInferenceRequest* request, ::tensorflow::serving::MultiInferenceResponse* response) { return this->MultiInference(context, request, response); }));}
    void SetMessageAllocatorFor_MultiInference(
        ::grpc::experimental::MessageAllocator< ::tensorflow::serving::MultiInferenceRequest, ::tensorflow::serving::MultiInferenceResponse>* allocator) {
    #ifdef GRPC_CALLBACK_API_NONEXPERIMENTAL
      ::grpc::internal::MethodHandler* const handler = ::grpc::Service::GetHandler(3);
    #else
      ::grpc::internal::MethodHandler* const handler = ::grpc::Service::experimental().GetHandler(3);
    #endif
      static_cast<::grpc_impl::internal::CallbackUnaryHandler< ::tensorflow::serving::MultiInferenceRequest, ::tensorflow::serving::MultiInferenceResponse>*>(handler)
              ->SetMessageAllocator(allocator);
    }
    ~ExperimentalWithCallbackMethod_MultiInference() override {
      BaseClassMustBeDerivedFromService(this);
    }
    // disable synchronous version of this method
    ::grpc::Status MultiInference(::grpc::ServerContext* /*context*/, const ::tensorflow::serving::MultiInferenceRequest* /*request*/, ::tensorflow::serving::MultiInferenceResponse* /*response*/) override {
      abort();
      return ::grpc::Status(::grpc::StatusCode::UNIMPLEMENTED, "");
    }
    #ifdef GRPC_CALLBACK_API_NONEXPERIMENTAL
    virtual ::grpc::ServerUnaryReactor* MultiInference(
      ::grpc::CallbackServerContext* /*context*/, const ::tensorflow::serving::MultiInferenceRequest* /*request*/, ::tensorflow::serving::MultiInferenceResponse* /*response*/)
    #else
    virtual ::grpc::experimental::ServerUnaryReactor* MultiInference(
      ::grpc::experimental::CallbackServerContext* /*context*/, const ::tensorflow::serving::MultiInferenceRequest* /*request*/, ::tensorflow::serving::MultiInferenceResponse* /*response*/)
    #endif
      { return nullptr; }
  };
  template <class BaseClass>
  class ExperimentalWithCallbackMethod_GetModelMetadata : public BaseClass {
   private:
    void BaseClassMustBeDerivedFromService(const Service* /*service*/) {}
   public:
    ExperimentalWithCallbackMethod_GetModelMetadata() {
    #ifdef GRPC_CALLBACK_API_NONEXPERIMENTAL
      ::grpc::Service::
    #else
      ::grpc::Service::experimental().
    #endif
        MarkMethodCallback(4,
          new ::grpc_impl::internal::CallbackUnaryHandler< ::tensorflow::serving::GetModelMetadataRequest, ::tensorflow::serving::GetModelMetadataResponse>(
            [this](
    #ifdef GRPC_CALLBACK_API_NONEXPERIMENTAL
                   ::grpc::CallbackServerContext*
    #else
                   ::grpc::experimental::CallbackServerContext*
    #endif
                     context, const ::tensorflow::serving::GetModelMetadataRequest* request, ::tensorflow::serving::GetModelMetadataResponse* response) { return this->GetModelMetadata(context, request, response); }));}
    void SetMessageAllocatorFor_GetModelMetadata(
        ::grpc::experimental::MessageAllocator< ::tensorflow::serving::GetModelMetadataRequest, ::tensorflow::serving::GetModelMetadataResponse>* allocator) {
    #ifdef GRPC_CALLBACK_API_NONEXPERIMENTAL
      ::grpc::internal::MethodHandler* const handler = ::grpc::Service::GetHandler(4);
    #else
      ::grpc::internal::MethodHandler* const handler = ::grpc::Service::experimental().GetHandler(4);
    #endif
      static_cast<::grpc_impl::internal::CallbackUnaryHandler< ::tensorflow::serving::GetModelMetadataRequest, ::tensorflow::serving::GetModelMetadataResponse>*>(handler)
              ->SetMessageAllocator(allocator);
    }
    ~ExperimentalWithCallbackMethod_GetModelMetadata() override {
      BaseClassMustBeDerivedFromService(this);
    }
    // disable synchronous version of this method
    ::grpc::Status GetModelMetadata(::grpc::ServerContext* /*context*/, const ::tensorflow::serving::GetModelMetadataRequest* /*request*/, ::tensorflow::serving::GetModelMetadataResponse* /*response*/) override {
      abort();
      return ::grpc::Status(::grpc::StatusCode::UNIMPLEMENTED, "");
    }
    #ifdef GRPC_CALLBACK_API_NONEXPERIMENTAL
    virtual ::grpc::ServerUnaryReactor* GetModelMetadata(
      ::grpc::CallbackServerContext* /*context*/, const ::tensorflow::serving::GetModelMetadataRequest* /*request*/, ::tensorflow::serving::GetModelMetadataResponse* /*response*/)
    #else
    virtual ::grpc::experimental::ServerUnaryReactor* GetModelMetadata(
      ::grpc::experimental::CallbackServerContext* /*context*/, const ::tensorflow::serving::GetModelMetadataRequest* /*request*/, ::tensorflow::serving::GetModelMetadataResponse* /*response*/)
    #endif
      { return nullptr; }
  };
  #ifdef GRPC_CALLBACK_API_NONEXPERIMENTAL
  typedef ExperimentalWithCallbackMethod_Classify<ExperimentalWithCallbackMethod_Regress<ExperimentalWithCallbackMethod_Predict<ExperimentalWithCallbackMethod_MultiInference<ExperimentalWithCallbackMethod_GetModelMetadata<Service > > > > > CallbackService;
  #endif

  typedef ExperimentalWithCallbackMethod_Classify<ExperimentalWithCallbackMethod_Regress<ExperimentalWithCallbackMethod_Predict<ExperimentalWithCallbackMethod_MultiInference<ExperimentalWithCallbackMethod_GetModelMetadata<Service > > > > > ExperimentalCallbackService;
  template <class BaseClass>
  class WithGenericMethod_Classify : public BaseClass {
   private:
    void BaseClassMustBeDerivedFromService(const Service* /*service*/) {}
   public:
    WithGenericMethod_Classify() {
      ::grpc::Service::MarkMethodGeneric(0);
    }
    ~WithGenericMethod_Classify() override {
      BaseClassMustBeDerivedFromService(this);
    }
    // disable synchronous version of this method
    ::grpc::Status Classify(::grpc::ServerContext* /*context*/, const ::tensorflow::serving::ClassificationRequest* /*request*/, ::tensorflow::serving::ClassificationResponse* /*response*/) override {
      abort();
      return ::grpc::Status(::grpc::StatusCode::UNIMPLEMENTED, "");
    }
  };
  template <class BaseClass>
  class WithGenericMethod_Regress : public BaseClass {
   private:
    void BaseClassMustBeDerivedFromService(const Service* /*service*/) {}
   public:
    WithGenericMethod_Regress() {
      ::grpc::Service::MarkMethodGeneric(1);
    }
    ~WithGenericMethod_Regress() override {
      BaseClassMustBeDerivedFromService(this);
    }
    // disable synchronous version of this method
    ::grpc::Status Regress(::grpc::ServerContext* /*context*/, const ::tensorflow::serving::RegressionRequest* /*request*/, ::tensorflow::serving::RegressionResponse* /*response*/) override {
      abort();
      return ::grpc::Status(::grpc::StatusCode::UNIMPLEMENTED, "");
    }
  };
  template <class BaseClass>
  class WithGenericMethod_Predict : public BaseClass {
   private:
    void BaseClassMustBeDerivedFromService(const Service* /*service*/) {}
   public:
    WithGenericMethod_Predict() {
      ::grpc::Service::MarkMethodGeneric(2);
    }
    ~WithGenericMethod_Predict() override {
      BaseClassMustBeDerivedFromService(this);
    }
    // disable synchronous version of this method
    ::grpc::Status Predict(::grpc::ServerContext* /*context*/, const ::tensorflow::serving::PredictRequest* /*request*/, ::tensorflow::serving::PredictResponse* /*response*/) override {
      abort();
      return ::grpc::Status(::grpc::StatusCode::UNIMPLEMENTED, "");
    }
  };
  template <class BaseClass>
  class WithGenericMethod_MultiInference : public BaseClass {
   private:
    void BaseClassMustBeDerivedFromService(const Service* /*service*/) {}
   public:
    WithGenericMethod_MultiInference() {
      ::grpc::Service::MarkMethodGeneric(3);
    }
    ~WithGenericMethod_MultiInference() override {
      BaseClassMustBeDerivedFromService(this);
    }
    // disable synchronous version of this method
    ::grpc::Status MultiInference(::grpc::ServerContext* /*context*/, const ::tensorflow::serving::MultiInferenceRequest* /*request*/, ::tensorflow::serving::MultiInferenceResponse* /*response*/) override {
      abort();
      return ::grpc::Status(::grpc::StatusCode::UNIMPLEMENTED, "");
    }
  };
  template <class BaseClass>
  class WithGenericMethod_GetModelMetadata : public BaseClass {
   private:
    void BaseClassMustBeDerivedFromService(const Service* /*service*/) {}
   public:
    WithGenericMethod_GetModelMetadata() {
      ::grpc::Service::MarkMethodGeneric(4);
    }
    ~WithGenericMethod_GetModelMetadata() override {
      BaseClassMustBeDerivedFromService(this);
    }
    // disable synchronous version of this method
    ::grpc::Status GetModelMetadata(::grpc::ServerContext* /*context*/, const ::tensorflow::serving::GetModelMetadataRequest* /*request*/, ::tensorflow::serving::GetModelMetadataResponse* /*response*/) override {
      abort();
      return ::grpc::Status(::grpc::StatusCode::UNIMPLEMENTED, "");
    }
  };
  template <class BaseClass>
  class WithRawMethod_Classify : public BaseClass {
   private:
    void BaseClassMustBeDerivedFromService(const Service* /*service*/) {}
   public:
    WithRawMethod_Classify() {
      ::grpc::Service::MarkMethodRaw(0);
    }
    ~WithRawMethod_Classify() override {
      BaseClassMustBeDerivedFromService(this);
    }
    // disable synchronous version of this method
    ::grpc::Status Classify(::grpc::ServerContext* /*context*/, const ::tensorflow::serving::ClassificationRequest* /*request*/, ::tensorflow::serving::ClassificationResponse* /*response*/) override {
      abort();
      return ::grpc::Status(::grpc::StatusCode::UNIMPLEMENTED, "");
    }
    void RequestClassify(::grpc::ServerContext* context, ::grpc::ByteBuffer* request, ::grpc::ServerAsyncResponseWriter< ::grpc::ByteBuffer>* response, ::grpc::CompletionQueue* new_call_cq, ::grpc::ServerCompletionQueue* notification_cq, void *tag) {
      ::grpc::Service::RequestAsyncUnary(0, context, request, response, new_call_cq, notification_cq, tag);
    }
  };
  template <class BaseClass>
  class WithRawMethod_Regress : public BaseClass {
   private:
    void BaseClassMustBeDerivedFromService(const Service* /*service*/) {}
   public:
    WithRawMethod_Regress() {
      ::grpc::Service::MarkMethodRaw(1);
    }
    ~WithRawMethod_Regress() override {
      BaseClassMustBeDerivedFromService(this);
    }
    // disable synchronous version of this method
    ::grpc::Status Regress(::grpc::ServerContext* /*context*/, const ::tensorflow::serving::RegressionRequest* /*request*/, ::tensorflow::serving::RegressionResponse* /*response*/) override {
      abort();
      return ::grpc::Status(::grpc::StatusCode::UNIMPLEMENTED, "");
    }
    void RequestRegress(::grpc::ServerContext* context, ::grpc::ByteBuffer* request, ::grpc::ServerAsyncResponseWriter< ::grpc::ByteBuffer>* response, ::grpc::CompletionQueue* new_call_cq, ::grpc::ServerCompletionQueue* notification_cq, void *tag) {
      ::grpc::Service::RequestAsyncUnary(1, context, request, response, new_call_cq, notification_cq, tag);
    }
  };
  template <class BaseClass>
  class WithRawMethod_Predict : public BaseClass {
   private:
    void BaseClassMustBeDerivedFromService(const Service* /*service*/) {}
   public:
    WithRawMethod_Predict() {
      ::grpc::Service::MarkMethodRaw(2);
    }
    ~WithRawMethod_Predict() override {
      BaseClassMustBeDerivedFromService(this);
    }
    // disable synchronous version of this method
    ::grpc::Status Predict(::grpc::ServerContext* /*context*/, const ::tensorflow::serving::PredictRequest* /*request*/, ::tensorflow::serving::PredictResponse* /*response*/) override {
      abort();
      return ::grpc::Status(::grpc::StatusCode::UNIMPLEMENTED, "");
    }
    void RequestPredict(::grpc::ServerContext* context, ::grpc::ByteBuffer* request, ::grpc::ServerAsyncResponseWriter< ::grpc::ByteBuffer>* response, ::grpc::CompletionQueue* new_call_cq, ::grpc::ServerCompletionQueue* notification_cq, void *tag) {
      ::grpc::Service::RequestAsyncUnary(2, context, request, response, new_call_cq, notification_cq, tag);
    }
  };
  template <class BaseClass>
  class WithRawMethod_MultiInference : public BaseClass {
   private:
    void BaseClassMustBeDerivedFromService(const Service* /*service*/) {}
   public:
    WithRawMethod_MultiInference() {
      ::grpc::Service::MarkMethodRaw(3);
    }
    ~WithRawMethod_MultiInference() override {
      BaseClassMustBeDerivedFromService(this);
    }
    // disable synchronous version of this method
    ::grpc::Status MultiInference(::grpc::ServerContext* /*context*/, const ::tensorflow::serving::MultiInferenceRequest* /*request*/, ::tensorflow::serving::MultiInferenceResponse* /*response*/) override {
      abort();
      return ::grpc::Status(::grpc::StatusCode::UNIMPLEMENTED, "");
    }
    void RequestMultiInference(::grpc::ServerContext* context, ::grpc::ByteBuffer* request, ::grpc::ServerAsyncResponseWriter< ::grpc::ByteBuffer>* response, ::grpc::CompletionQueue* new_call_cq, ::grpc::ServerCompletionQueue* notification_cq, void *tag) {
      ::grpc::Service::RequestAsyncUnary(3, context, request, response, new_call_cq, notification_cq, tag);
    }
  };
  template <class BaseClass>
  class WithRawMethod_GetModelMetadata : public BaseClass {
   private:
    void BaseClassMustBeDerivedFromService(const Service* /*service*/) {}
   public:
    WithRawMethod_GetModelMetadata() {
      ::grpc::Service::MarkMethodRaw(4);
    }
    ~WithRawMethod_GetModelMetadata() override {
      BaseClassMustBeDerivedFromService(this);
    }
    // disable synchronous version of this method
    ::grpc::Status GetModelMetadata(::grpc::ServerContext* /*context*/, const ::tensorflow::serving::GetModelMetadataRequest* /*request*/, ::tensorflow::serving::GetModelMetadataResponse* /*response*/) override {
      abort();
      return ::grpc::Status(::grpc::StatusCode::UNIMPLEMENTED, "");
    }
    void RequestGetModelMetadata(::grpc::ServerContext* context, ::grpc::ByteBuffer* request, ::grpc::ServerAsyncResponseWriter< ::grpc::ByteBuffer>* response, ::grpc::CompletionQueue* new_call_cq, ::grpc::ServerCompletionQueue* notification_cq, void *tag) {
      ::grpc::Service::RequestAsyncUnary(4, context, request, response, new_call_cq, notification_cq, tag);
    }
  };
  template <class BaseClass>
  class ExperimentalWithRawCallbackMethod_Classify : public BaseClass {
   private:
    void BaseClassMustBeDerivedFromService(const Service* /*service*/) {}
   public:
    ExperimentalWithRawCallbackMethod_Classify() {
    #ifdef GRPC_CALLBACK_API_NONEXPERIMENTAL
      ::grpc::Service::
    #else
      ::grpc::Service::experimental().
    #endif
        MarkMethodRawCallback(0,
          new ::grpc_impl::internal::CallbackUnaryHandler< ::grpc::ByteBuffer, ::grpc::ByteBuffer>(
            [this](
    #ifdef GRPC_CALLBACK_API_NONEXPERIMENTAL
                   ::grpc::CallbackServerContext*
    #else
                   ::grpc::experimental::CallbackServerContext*
    #endif
                     context, const ::grpc::ByteBuffer* request, ::grpc::ByteBuffer* response) { return this->Classify(context, request, response); }));
    }
    ~ExperimentalWithRawCallbackMethod_Classify() override {
      BaseClassMustBeDerivedFromService(this);
    }
    // disable synchronous version of this method
    ::grpc::Status Classify(::grpc::ServerContext* /*context*/, const ::tensorflow::serving::ClassificationRequest* /*request*/, ::tensorflow::serving::ClassificationResponse* /*response*/) override {
      abort();
      return ::grpc::Status(::grpc::StatusCode::UNIMPLEMENTED, "");
    }
    #ifdef GRPC_CALLBACK_API_NONEXPERIMENTAL
    virtual ::grpc::ServerUnaryReactor* Classify(
      ::grpc::CallbackServerContext* /*context*/, const ::grpc::ByteBuffer* /*request*/, ::grpc::ByteBuffer* /*response*/)
    #else
    virtual ::grpc::experimental::ServerUnaryReactor* Classify(
      ::grpc::experimental::CallbackServerContext* /*context*/, const ::grpc::ByteBuffer* /*request*/, ::grpc::ByteBuffer* /*response*/)
    #endif
      { return nullptr; }
  };
  template <class BaseClass>
  class ExperimentalWithRawCallbackMethod_Regress : public BaseClass {
   private:
    void BaseClassMustBeDerivedFromService(const Service* /*service*/) {}
   public:
    ExperimentalWithRawCallbackMethod_Regress() {
    #ifdef GRPC_CALLBACK_API_NONEXPERIMENTAL
      ::grpc::Service::
    #else
      ::grpc::Service::experimental().
    #endif
        MarkMethodRawCallback(1,
          new ::grpc_impl::internal::CallbackUnaryHandler< ::grpc::ByteBuffer, ::grpc::ByteBuffer>(
            [this](
    #ifdef GRPC_CALLBACK_API_NONEXPERIMENTAL
                   ::grpc::CallbackServerContext*
    #else
                   ::grpc::experimental::CallbackServerContext*
    #endif
                     context, const ::grpc::ByteBuffer* request, ::grpc::ByteBuffer* response) { return this->Regress(context, request, response); }));
    }
    ~ExperimentalWithRawCallbackMethod_Regress() override {
      BaseClassMustBeDerivedFromService(this);
    }
    // disable synchronous version of this method
    ::grpc::Status Regress(::grpc::ServerContext* /*context*/, const ::tensorflow::serving::RegressionRequest* /*request*/, ::tensorflow::serving::RegressionResponse* /*response*/) override {
      abort();
      return ::grpc::Status(::grpc::StatusCode::UNIMPLEMENTED, "");
    }
    #ifdef GRPC_CALLBACK_API_NONEXPERIMENTAL
    virtual ::grpc::ServerUnaryReactor* Regress(
      ::grpc::CallbackServerContext* /*context*/, const ::grpc::ByteBuffer* /*request*/, ::grpc::ByteBuffer* /*response*/)
    #else
    virtual ::grpc::experimental::ServerUnaryReactor* Regress(
      ::grpc::experimental::CallbackServerContext* /*context*/, const ::grpc::ByteBuffer* /*request*/, ::grpc::ByteBuffer* /*response*/)
    #endif
      { return nullptr; }
  };
  template <class BaseClass>
  class ExperimentalWithRawCallbackMethod_Predict : public BaseClass {
   private:
    void BaseClassMustBeDerivedFromService(const Service* /*service*/) {}
   public:
    ExperimentalWithRawCallbackMethod_Predict() {
    #ifdef GRPC_CALLBACK_API_NONEXPERIMENTAL
      ::grpc::Service::
    #else
      ::grpc::Service::experimental().
    #endif
        MarkMethodRawCallback(2,
          new ::grpc_impl::internal::CallbackUnaryHandler< ::grpc::ByteBuffer, ::grpc::ByteBuffer>(
            [this](
    #ifdef GRPC_CALLBACK_API_NONEXPERIMENTAL
                   ::grpc::CallbackServerContext*
    #else
                   ::grpc::experimental::CallbackServerContext*
    #endif
                     context, const ::grpc::ByteBuffer* request, ::grpc::ByteBuffer* response) { return this->Predict(context, request, response); }));
    }
    ~ExperimentalWithRawCallbackMethod_Predict() override {
      BaseClassMustBeDerivedFromService(this);
    }
    // disable synchronous version of this method
    ::grpc::Status Predict(::grpc::ServerContext* /*context*/, const ::tensorflow::serving::PredictRequest* /*request*/, ::tensorflow::serving::PredictResponse* /*response*/) override {
      abort();
      return ::grpc::Status(::grpc::StatusCode::UNIMPLEMENTED, "");
    }
    #ifdef GRPC_CALLBACK_API_NONEXPERIMENTAL
    virtual ::grpc::ServerUnaryReactor* Predict(
      ::grpc::CallbackServerContext* /*context*/, const ::grpc::ByteBuffer* /*request*/, ::grpc::ByteBuffer* /*response*/)
    #else
    virtual ::grpc::experimental::ServerUnaryReactor* Predict(
      ::grpc::experimental::CallbackServerContext* /*context*/, const ::grpc::ByteBuffer* /*request*/, ::grpc::ByteBuffer* /*response*/)
    #endif
      { return nullptr; }
  };
  template <class BaseClass>
  class ExperimentalWithRawCallbackMethod_MultiInference : public BaseClass {
   private:
    void BaseClassMustBeDerivedFromService(const Service* /*service*/) {}
   public:
    ExperimentalWithRawCallbackMethod_MultiInference() {
    #ifdef GRPC_CALLBACK_API_NONEXPERIMENTAL
      ::grpc::Service::
    #else
      ::grpc::Service::experimental().
    #endif
        MarkMethodRawCallback(3,
          new ::grpc_impl::internal::CallbackUnaryHandler< ::grpc::ByteBuffer, ::grpc::ByteBuffer>(
            [this](
    #ifdef GRPC_CALLBACK_API_NONEXPERIMENTAL
                   ::grpc::CallbackServerContext*
    #else
                   ::grpc::experimental::CallbackServerContext*
    #endif
                     context, const ::grpc::ByteBuffer* request, ::grpc::ByteBuffer* response) { return this->MultiInference(context, request, response); }));
    }
    ~ExperimentalWithRawCallbackMethod_MultiInference() override {
      BaseClassMustBeDerivedFromService(this);
    }
    // disable synchronous version of this method
    ::grpc::Status MultiInference(::grpc::ServerContext* /*context*/, const ::tensorflow::serving::MultiInferenceRequest* /*request*/, ::tensorflow::serving::MultiInferenceResponse* /*response*/) override {
      abort();
      return ::grpc::Status(::grpc::StatusCode::UNIMPLEMENTED, "");
    }
    #ifdef GRPC_CALLBACK_API_NONEXPERIMENTAL
    virtual ::grpc::ServerUnaryReactor* MultiInference(
      ::grpc::CallbackServerContext* /*context*/, const ::grpc::ByteBuffer* /*request*/, ::grpc::ByteBuffer* /*response*/)
    #else
    virtual ::grpc::experimental::ServerUnaryReactor* MultiInference(
      ::grpc::experimental::CallbackServerContext* /*context*/, const ::grpc::ByteBuffer* /*request*/, ::grpc::ByteBuffer* /*response*/)
    #endif
      { return nullptr; }
  };
  template <class BaseClass>
  class ExperimentalWithRawCallbackMethod_GetModelMetadata : public BaseClass {
   private:
    void BaseClassMustBeDerivedFromService(const Service* /*service*/) {}
   public:
    ExperimentalWithRawCallbackMethod_GetModelMetadata() {
    #ifdef GRPC_CALLBACK_API_NONEXPERIMENTAL
      ::grpc::Service::
    #else
      ::grpc::Service::experimental().
    #endif
        MarkMethodRawCallback(4,
          new ::grpc_impl::internal::CallbackUnaryHandler< ::grpc::ByteBuffer, ::grpc::ByteBuffer>(
            [this](
    #ifdef GRPC_CALLBACK_API_NONEXPERIMENTAL
                   ::grpc::CallbackServerContext*
    #else
                   ::grpc::experimental::CallbackServerContext*
    #endif
                     context, const ::grpc::ByteBuffer* request, ::grpc::ByteBuffer* response) { return this->GetModelMetadata(context, request, response); }));
    }
    ~ExperimentalWithRawCallbackMethod_GetModelMetadata() override {
      BaseClassMustBeDerivedFromService(this);
    }
    // disable synchronous version of this method
    ::grpc::Status GetModelMetadata(::grpc::ServerContext* /*context*/, const ::tensorflow::serving::GetModelMetadataRequest* /*request*/, ::tensorflow::serving::GetModelMetadataResponse* /*response*/) override {
      abort();
      return ::grpc::Status(::grpc::StatusCode::UNIMPLEMENTED, "");
    }
    #ifdef GRPC_CALLBACK_API_NONEXPERIMENTAL
    virtual ::grpc::ServerUnaryReactor* GetModelMetadata(
      ::grpc::CallbackServerContext* /*context*/, const ::grpc::ByteBuffer* /*request*/, ::grpc::ByteBuffer* /*response*/)
    #else
    virtual ::grpc::experimental::ServerUnaryReactor* GetModelMetadata(
      ::grpc::experimental::CallbackServerContext* /*context*/, const ::grpc::ByteBuffer* /*request*/, ::grpc::ByteBuffer* /*response*/)
    #endif
      { return nullptr; }
  };
  template <class BaseClass>
  class WithStreamedUnaryMethod_Classify : public BaseClass {
   private:
    void BaseClassMustBeDerivedFromService(const Service* /*service*/) {}
   public:
    WithStreamedUnaryMethod_Classify() {
      ::grpc::Service::MarkMethodStreamed(0,
        new ::grpc::internal::StreamedUnaryHandler< ::tensorflow::serving::ClassificationRequest, ::tensorflow::serving::ClassificationResponse>(std::bind(&WithStreamedUnaryMethod_Classify<BaseClass>::StreamedClassify, this, std::placeholders::_1, std::placeholders::_2)));
    }
    ~WithStreamedUnaryMethod_Classify() override {
      BaseClassMustBeDerivedFromService(this);
    }
    // disable regular version of this method
    ::grpc::Status Classify(::grpc::ServerContext* /*context*/, const ::tensorflow::serving::ClassificationRequest* /*request*/, ::tensorflow::serving::ClassificationResponse* /*response*/) override {
      abort();
      return ::grpc::Status(::grpc::StatusCode::UNIMPLEMENTED, "");
    }
    // replace default version of method with streamed unary
    virtual ::grpc::Status StreamedClassify(::grpc::ServerContext* context, ::grpc::ServerUnaryStreamer< ::tensorflow::serving::ClassificationRequest,::tensorflow::serving::ClassificationResponse>* server_unary_streamer) = 0;
  };
  template <class BaseClass>
  class WithStreamedUnaryMethod_Regress : public BaseClass {
   private:
    void BaseClassMustBeDerivedFromService(const Service* /*service*/) {}
   public:
    WithStreamedUnaryMethod_Regress() {
      ::grpc::Service::MarkMethodStreamed(1,
        new ::grpc::internal::StreamedUnaryHandler< ::tensorflow::serving::RegressionRequest, ::tensorflow::serving::RegressionResponse>(std::bind(&WithStreamedUnaryMethod_Regress<BaseClass>::StreamedRegress, this, std::placeholders::_1, std::placeholders::_2)));
    }
    ~WithStreamedUnaryMethod_Regress() override {
      BaseClassMustBeDerivedFromService(this);
    }
    // disable regular version of this method
    ::grpc::Status Regress(::grpc::ServerContext* /*context*/, const ::tensorflow::serving::RegressionRequest* /*request*/, ::tensorflow::serving::RegressionResponse* /*response*/) override {
      abort();
      return ::grpc::Status(::grpc::StatusCode::UNIMPLEMENTED, "");
    }
    // replace default version of method with streamed unary
    virtual ::grpc::Status StreamedRegress(::grpc::ServerContext* context, ::grpc::ServerUnaryStreamer< ::tensorflow::serving::RegressionRequest,::tensorflow::serving::RegressionResponse>* server_unary_streamer) = 0;
  };
  template <class BaseClass>
  class WithStreamedUnaryMethod_Predict : public BaseClass {
   private:
    void BaseClassMustBeDerivedFromService(const Service* /*service*/) {}
   public:
    WithStreamedUnaryMethod_Predict() {
      ::grpc::Service::MarkMethodStreamed(2,
        new ::grpc::internal::StreamedUnaryHandler< ::tensorflow::serving::PredictRequest, ::tensorflow::serving::PredictResponse>(std::bind(&WithStreamedUnaryMethod_Predict<BaseClass>::StreamedPredict, this, std::placeholders::_1, std::placeholders::_2)));
    }
    ~WithStreamedUnaryMethod_Predict() override {
      BaseClassMustBeDerivedFromService(this);
    }
    // disable regular version of this method
    ::grpc::Status Predict(::grpc::ServerContext* /*context*/, const ::tensorflow::serving::PredictRequest* /*request*/, ::tensorflow::serving::PredictResponse* /*response*/) override {
      abort();
      return ::grpc::Status(::grpc::StatusCode::UNIMPLEMENTED, "");
    }
    // replace default version of method with streamed unary
    virtual ::grpc::Status StreamedPredict(::grpc::ServerContext* context, ::grpc::ServerUnaryStreamer< ::tensorflow::serving::PredictRequest,::tensorflow::serving::PredictResponse>* server_unary_streamer) = 0;
  };
  template <class BaseClass>
  class WithStreamedUnaryMethod_MultiInference : public BaseClass {
   private:
    void BaseClassMustBeDerivedFromService(const Service* /*service*/) {}
   public:
    WithStreamedUnaryMethod_MultiInference() {
      ::grpc::Service::MarkMethodStreamed(3,
        new ::grpc::internal::StreamedUnaryHandler< ::tensorflow::serving::MultiInferenceRequest, ::tensorflow::serving::MultiInferenceResponse>(std::bind(&WithStreamedUnaryMethod_MultiInference<BaseClass>::StreamedMultiInference, this, std::placeholders::_1, std::placeholders::_2)));
    }
    ~WithStreamedUnaryMethod_MultiInference() override {
      BaseClassMustBeDerivedFromService(this);
    }
    // disable regular version of this method
    ::grpc::Status MultiInference(::grpc::ServerContext* /*context*/, const ::tensorflow::serving::MultiInferenceRequest* /*request*/, ::tensorflow::serving::MultiInferenceResponse* /*response*/) override {
      abort();
      return ::grpc::Status(::grpc::StatusCode::UNIMPLEMENTED, "");
    }
    // replace default version of method with streamed unary
    virtual ::grpc::Status StreamedMultiInference(::grpc::ServerContext* context, ::grpc::ServerUnaryStreamer< ::tensorflow::serving::MultiInferenceRequest,::tensorflow::serving::MultiInferenceResponse>* server_unary_streamer) = 0;
  };
  template <class BaseClass>
  class WithStreamedUnaryMethod_GetModelMetadata : public BaseClass {
   private:
    void BaseClassMustBeDerivedFromService(const Service* /*service*/) {}
   public:
    WithStreamedUnaryMethod_GetModelMetadata() {
      ::grpc::Service::MarkMethodStreamed(4,
        new ::grpc::internal::StreamedUnaryHandler< ::tensorflow::serving::GetModelMetadataRequest, ::tensorflow::serving::GetModelMetadataResponse>(std::bind(&WithStreamedUnaryMethod_GetModelMetadata<BaseClass>::StreamedGetModelMetadata, this, std::placeholders::_1, std::placeholders::_2)));
    }
    ~WithStreamedUnaryMethod_GetModelMetadata() override {
      BaseClassMustBeDerivedFromService(this);
    }
    // disable regular version of this method
    ::grpc::Status GetModelMetadata(::grpc::ServerContext* /*context*/, const ::tensorflow::serving::GetModelMetadataRequest* /*request*/, ::tensorflow::serving::GetModelMetadataResponse* /*response*/) override {
      abort();
      return ::grpc::Status(::grpc::StatusCode::UNIMPLEMENTED, "");
    }
    // replace default version of method with streamed unary
    virtual ::grpc::Status StreamedGetModelMetadata(::grpc::ServerContext* context, ::grpc::ServerUnaryStreamer< ::tensorflow::serving::GetModelMetadataRequest,::tensorflow::serving::GetModelMetadataResponse>* server_unary_streamer) = 0;
  };
  typedef WithStreamedUnaryMethod_Classify<WithStreamedUnaryMethod_Regress<WithStreamedUnaryMethod_Predict<WithStreamedUnaryMethod_MultiInference<WithStreamedUnaryMethod_GetModelMetadata<Service > > > > > StreamedUnaryService;
  typedef Service SplitStreamedService;
  typedef WithStreamedUnaryMethod_Classify<WithStreamedUnaryMethod_Regress<WithStreamedUnaryMethod_Predict<WithStreamedUnaryMethod_MultiInference<WithStreamedUnaryMethod_GetModelMetadata<Service > > > > > StreamedService;
};

}  // namespace serving
}  // namespace tensorflow


#endif  // GRPC_tensorflow_5fserving_2fapis_2fprediction_5fservice_2eproto__INCLUDED
